{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'q'\n"
     ]
    }
   ],
   "source": [
    "pip install -rq requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "# import qgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41, 23), (120, 11))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = pd.read_csv('emissions_profile_data.csv')\n",
    "df_2 = pd.read_csv('historic_emissions_data.csv')\n",
    "df_1.shape, df_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a description of the steps to be taken to clean and transform the data beforem\n",
    "\n",
    "1. Combine emissions data with the historic data to work on one dataframe\n",
    "1.5 Rename certain columns to make them easier to work with\n",
    "\n",
    "Data Cleaning Steps:\n",
    "\n",
    "2. Replace all terms that represent missing data with NaN\n",
    "3. Check all year columns are 4 numbers long, if not replace with 0 for now\n",
    "4. Check all emissions columns are numbers and positive\n",
    "\n",
    "Data Transformation Steps:\n",
    "\n",
    "5. Replace Net Zero target year with 2050 if it is not present\n",
    "6. Replace baseline year of emmissons with most recent year of emmissions if it is not present\n",
    "7. Create a new columns called Net Zero target == 0.99*Baseline year emmissions for each scope\n",
    "8. Create new columns called scope 1, scope 2 and scope 3 emissions on the interim year == (1-interim target)*baseline year emissions\n",
    "\n",
    "9. Remove rows where there is no baseline emmission or target information for any of the scopes\n",
    "\n",
    "\n",
    "From here we can carry out each question in the assignment\n",
    "\n",
    "a) Design a data pipeline to cleanse the raw data and produce a forecast for yearly\n",
    "carbon emissions from Baseline year to 2050. The forecast should follow a linear\n",
    "yearly decrease between baseline emissions and both interim target years, and\n",
    "ultimate net zero year. The forecast should be produced for each firm for each scope,\n",
    "as displayed in example output (A).\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 - Merging the two dataframes - Done\n",
      "Index(['verdantix_id', 'Scope 1 Emissions Year', 'Scope 1 Emissions',\n",
      "       'Scope 2 Emissions Year', 'Scope 2 - Location', 'Scope 2 ',\n",
      "       'Scope 2 - market', 'Scope 3', 'Baseline Year (Scope 1)',\n",
      "       'Base Year Emissions - Scope 1', 'Baseline Year (Scope 2)',\n",
      "       'Base Year Emissions - Scope 2', 'Baseline Year (Scope 3)',\n",
      "       'Base Year Emissions - Scope 3', 'NZ Target Year: Scope 1',\n",
      "       'NZ Target Year: Scope 2', 'NZ Target Year: Scope 3',\n",
      "       'Interim Target Year 1: Scope 1', 'Interim Target Year 2: Scope 2',\n",
      "       'Interim Target Year 2: Scope 3', 'Interim Target % 1: Scope 1',\n",
      "       'Interim Target % 1: Scope 2', 'Interim Target % 1: Scope 3',\n",
      "       'lookup_1', 'lookup_2', 'lookup_3', 'Scope _1', 'Scope _2', 'Scope _3',\n",
      "       'Actual/Projection_1', 'Actual/Projection_2', 'Actual/Projection_3',\n",
      "       '2016_1', '2016_2', '2016_3', '2017_1', '2017_2', '2017_3', '2018_1',\n",
      "       '2018_2', '2018_3', '2019_1', '2019_2', '2019_3', '2020_1', '2020_2',\n",
      "       '2020_3', '2021_1', '2021_2', '2021_3', '2022_1', '2022_2', '2022_3',\n",
      "       'Scope 3 Emissions Year'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 1. Combine emissions data with the historic data to work on one dataframe \"\"\"\n",
    "\n",
    "# Aggregate the data by verdantix_id\n",
    "df_2_agg = df_2.groupby('verdantix_id').agg(list).reset_index() \n",
    "\n",
    "# Expand the columns with lists into multiple columns\n",
    "expanded_columns = {}\n",
    "for col in df_2_agg.columns:\n",
    "    if col != 'verdantix_id': \n",
    "        max_len = df_2_agg[col].str.len().max() # Number of values in each columns list after aggregating ; should be 3\n",
    "        col_names = [f\"{col}_{i+1}\" for i in range(max_len)] \n",
    "        expanded_df = pd.DataFrame(df_2_agg[col].tolist(), columns=col_names)\n",
    "        expanded_columns[col] = expanded_df\n",
    "\n",
    "# Combine expanded columns into one DataFrame\n",
    "expanded_data = pd.concat([df_2_agg[['verdantix_id']]] + list(expanded_columns.values()), axis=1)\n",
    "\n",
    "# Step 3: Merge the expanded data with first_df\n",
    "merged_df = pd.merge(df_1, expanded_data, on='verdantix_id', how='left')\n",
    "\n",
    "print('Stage 1 - Merging the two dataframes - Done')\n",
    "\n",
    "# Rename columns to make them easier to work with\n",
    "merged_df.rename(columns={'Scope 2  Emissions': 'Scope 2 Emissions'}, inplace=True)\n",
    "\n",
    "merged_df['Scope 3 Emissions Year'] = merged_df['Scope 1 Emissions Year']\n",
    "\n",
    "# # Display the merged DataFrame\n",
    "print(merged_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 1.5 Rename certain columns to make them easier to work with \"\"\"\n",
    "\n",
    "column_name_change_list = [\n",
    "                           'Scope 2 - Location',\n",
    "                           'Scope 2 ',\n",
    "                           'Scope 2 - market', \n",
    "                           'Scope 3',\n",
    "                           '2016_1', \n",
    "                           '2016_2', \n",
    "                           '2016_3', \n",
    "                           '2017_1', \n",
    "                           '2017_2', \n",
    "                           '2017_3', \n",
    "                           '2018_1',\n",
    "                           '2018_2', \n",
    "                           '2018_3',\n",
    "                           '2019_1',\n",
    "                           '2019_2', \n",
    "                           '2019_3', \n",
    "                           '2020_1', \n",
    "                           '2020_2',\n",
    "                           '2020_3', \n",
    "                           '2021_1', \n",
    "                           '2021_2', \n",
    "                           '2021_3', \n",
    "                           '2022_1', \n",
    "                           '2022_2', \n",
    "                           '2022_3'\n",
    "]\n",
    "\n",
    "\n",
    "for col in column_name_change_list:\n",
    "    if col in merged_df.columns:\n",
    "        new_col_name = col + ' Emissions'\n",
    "        merged_df.rename(columns={col: new_col_name}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 - Missing values replaced with NaN - Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 2. Replace all terms that represent missing data with NaN \"\"\"\n",
    "\n",
    "def replace_values(df, old_value, new_value):\n",
    "\n",
    "    df = df.applymap(lambda x: new_value if isinstance(x, str) and (x.lower() == old_value.lower() or old_value.lower() in x.lower()) else x)\n",
    "    return df\n",
    "\n",
    "# Replace all occurrences of 'Not stated', 'Not disclosed', and 'None' with None in df_1\n",
    "merged_df = replace_values(merged_df, 'Not stated', None)\n",
    "merged_df = replace_values(merged_df, 'Not disclosed', None)\n",
    "merged_df = replace_values(merged_df, 'None', None)\n",
    "#print(merged_df)\n",
    "\n",
    "print('Stage 2 - Missing values replaced with NaN - Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test_1=merged_df[year_columns]\n",
    "\n",
    "# for column_name in test_1.columns:\n",
    "#     unique_values = test_1[column_name].unique()\n",
    "#     print(f\"Unique values in {column_name}: {unique_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 3 - Year columns are 4 numbers long, if not change to -1 for now - Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 3. Check all year columns are 4 numbers long, if not change to -1 for now \"\"\"\n",
    "\n",
    "# Get all columns with 'year' in their name\n",
    "year_columns = [col for col in merged_df.columns if 'year' in col.lower()]\n",
    "# Remove columns with 'emissions' in their name if after the word 'year'\n",
    "year_columns = [col for col in year_columns if 'emissions' not in col.lower() or 'year' in col.lower().split('emissions')[1]]\n",
    "\n",
    "# Convert year columns to object type\n",
    "merged_df[year_columns] = merged_df[year_columns].fillna(-1).astype(int)\n",
    "merged_df[year_columns] = merged_df[year_columns].astype(float)\n",
    "merged_df[year_columns] = merged_df[year_columns].astype(int)\n",
    "\n",
    "# Add 20 to the front of numbers between 15 and 50\n",
    "merged_df[year_columns] = merged_df[year_columns].applymap(lambda x: int(f\"20{x}\") if 15 <= x <= 50 else x)\n",
    "\n",
    "# Change any year values that are not 4 characters long to 0\n",
    "merged_df[year_columns] = merged_df[year_columns].applymap(lambda x: -1 if len(str(x)) != 4 else x)\n",
    "\n",
    "# change the year columns to object type\n",
    "merged_df[year_columns] = merged_df[year_columns].astype(object)\n",
    "\n",
    "print('Stage 3 - Year columns are 4 numbers long, if not change to -1 for now - Done')\n",
    "\n",
    "# # Check if all year columns are 4 numbers long\n",
    "# invalid_year_counts = merged_df[year_columns].applymap(lambda x: len(str(x)) != 4 if x is not None else False).sum()\n",
    "# print(invalid_year_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 4 - Emissions columns are numbers and positive - Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 4. Check all emissions columns are numbers and positive, if not change to -1 for now \"\"\"\n",
    "\n",
    "# Get all columns with 'year' in their name\n",
    "emissions_columns = [col for col in merged_df.columns if 'emissions' in col.lower()]\n",
    "\n",
    "# Remove columns with 'year' in their name if after the word 'emissions'\n",
    "emissions_columns = [col for col in emissions_columns if 'year' not in col.lower() or 'emissions' in col.lower().split('year')[1]]\n",
    "\n",
    "# Ensure all values are either positive, 0 or -1\n",
    "merged_df[emissions_columns] = merged_df[emissions_columns].applymap(lambda x: x if x >= 0 else -1)\n",
    "merged_df[emissions_columns] = merged_df[emissions_columns].astype(int)\n",
    "\n",
    "print('Stage 4 - Emissions columns are numbers and positive - Done')\n",
    "#print(emissions_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 5 - Net Zero target year replaced with 2050 if not present - Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 5. Replace Net Zero target year with 2050 if it is not present \"\"\"\n",
    "\n",
    "nz_target_columns = ['NZ Target Year: Scope 1', 'NZ Target Year: Scope 2', 'NZ Target Year: Scope 3']\n",
    "merged_df[nz_target_columns] = merged_df[nz_target_columns].replace(-1, 2050)\n",
    "\n",
    "print('Stage 5 - Net Zero target year replaced with 2050 if not present - Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Unique values in each column of df_1\n",
    "# for column_name in nz_target_columns:\n",
    "#     unique_values = merged_df[column_name].unique()\n",
    "#     print(f\"Unique values in {column_name}: {unique_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 6 - Baseline year of emmissons with most recent year of emmissions if it is not present - Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 6. Replace baseline year of emmissons with most recent year of emmissions if it is not present \"\"\"\n",
    "\n",
    "# baseline_year_columns = ['Baseline Year (Scope 1)', 'Baseline Year (Scope 2)', 'Baseline Year (Scope 3)']\n",
    "# year_columns_1 = [\n",
    "#     '2016_1 Emissions', \n",
    "#     '2017_1 Emissions', \n",
    "#     '2018_1 Emissions', \n",
    "#     '2019_1 Emissions', \n",
    "#     '2020_1 Emissions',\n",
    "#     '2021_1 Emissions', \n",
    "#     '2022_1 Emissions', \n",
    "# ][::-1]\n",
    "\n",
    "# year_columns_2 = [\n",
    "#     '2016_2 Emissions', \n",
    "#     '2017_2 Emissions', \n",
    "#     '2018_2 Emissions', \n",
    "#     '2019_2 Emissions', \n",
    "#     '2020_2 Emissions',\n",
    "#     '2021_2 Emissions', \n",
    "#     '2022_2 Emissions', \n",
    "# ][::-1]\n",
    "\n",
    "# year_columns_3 = [\n",
    "#     '2016_3 Emissions', \n",
    "#     '2017_3 Emissions', \n",
    "#     '2018_3 Emissions', \n",
    "#     '2019_3 Emissions', \n",
    "#     '2020_3 Emissions',\n",
    "#     '2021_3 Emissions', \n",
    "#     '2022_3 Emissions', \n",
    "# ][::-1]\n",
    "\n",
    "# for index,row in merged_df.iterrows():\n",
    "#     if row[baseline_year_columns[0]] == -1:\n",
    "#         non_negative_values = row[year_columns_1]\n",
    "#         most_recent_year = non_negative_values[non_negative_values != -1].index\n",
    "#         print(most_recent_year)\n",
    "#         if len(most_recent_year) > 0:\n",
    "#              merged_df.at[index, baseline_year_columns[0]] = row[most_recent_year[0]]\n",
    "\n",
    "print('Stage 6 - Baseline year of emmissons with most recent year of emmissions if it is not present - Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 7 - New columns called Net Zero target == 0.99*Baseline year emmissions for each scope - Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 7. Create a new columns called Net Zero target == 0.99*Baseline year emmissions for each scope \"\"\"\n",
    "\n",
    "base_year_emissions_columns = ['Base Year Emissions - Scope 1', 'Base Year Emissions - Scope 2', 'Base Year Emissions - Scope 3']\n",
    "for i, col in enumerate(base_year_emissions_columns):\n",
    "    #print(merged_df[col]*0.99)\n",
    "    merged_df[f'Net Zero Reduction Target Emissions - Scope {i+1}'] = 0.99 * merged_df[col]\n",
    "\n",
    "print('Stage 7 - New columns called Net Zero target == 0.99*Baseline year emmissions for each scope - Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 8. Create new columns called scope 1, scope 2 and scope 3 emissions on the interim year == (1-interim target)*baseline year emissions \"\"\"\n",
    "\n",
    "# Fix names of columns\n",
    "merged_df.rename(columns={\n",
    "    'Interim Target Year 2: Scope 2': 'Interim Target Year 1: Scope 2',\n",
    "    'Interim Target Year 2: Scope 3': 'Interim Target Year 1: Scope 3'\n",
    "}, inplace=True)\n",
    "\n",
    "# Correct interim target years if any of them is -1\n",
    "interim_target_columns = ['Interim Target Year 1: Scope 1', 'Interim Target Year 1: Scope 2', 'Interim Target Year 1: Scope 3']\n",
    "\n",
    "for col in interim_target_columns:\n",
    "    other_cols = [c for c in interim_target_columns if c != col]\n",
    "    merged_df[col] = merged_df.apply(lambda row: next((row[other_col] for other_col in other_cols if row[other_col] != -1), row[col]), axis=1)\n",
    "\n",
    "# Create new columns\n",
    "\n",
    "base_year_emissions_columns = ['Base Year Emissions - Scope 1', 'Base Year Emissions - Scope 2', 'Base Year Emissions - Scope 3']\n",
    "\n",
    "for i, col in enumerate(base_year_emissions_columns):\n",
    "    interim_target_col = f'Interim Target % 1: Scope {i+1}'\n",
    "    merged_df[f'Interim Target Emissions - Scope {i+1}'] = (1 - merged_df[interim_target_col]) * merged_df[col]\n",
    "\n",
    "#print('Stage 8 - New columns called interim target emissions for each scope - Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 9 - Remove rows where there is no baseline emission or target information for any of the scopes - Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 9. Remove rows where there is no baseline emmission or target information for any of the scopes \"\"\"\n",
    "\n",
    "baseline_emissions_columns = ['Base Year Emissions - Scope 1',\n",
    "                              'Base Year Emissions - Scope 2',\n",
    "                              'Base Year Emissions - Scope 3'\n",
    "                              ]\n",
    "\n",
    "target_info_columns = ['Interim Target Year 1: Scope 1',\n",
    "                       'Interim Target Year 1: Scope 2', \n",
    "                       'Interim Target Year 1: Scope 3'\n",
    "                       ]\n",
    "# Check each row if all baseline_emissions_columns are equal to -1\n",
    "for index, row in merged_df.iterrows():\n",
    "    if all(row[col] == -1 for col in baseline_emissions_columns):\n",
    "        # If all baseline_emissions_columns are -1, check target_info_columns\n",
    "        if all(row[col] == -1 for col in target_info_columns):\n",
    "            # If both baseline_emissions_columns and target_info_columns contain only -1, remove the row\n",
    "            merged_df.drop(index, inplace=True)\n",
    "\n",
    "print('Stage 9 - Remove rows where there is no baseline emission or target information for any of the scopes - Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nTask\\na) Design a data pipeline to cleanse the raw data and produce a forecast for yearly\\ncarbon emissions from Baseline year to 2050. The forecast should follow a linear\\nyearly decrease between baseline emissions and both interim target years, and\\nultimate net zero year. The forecast should be produced for each firm for each scope,\\nas displayed in example output (A).\\n'"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "Task\n",
    "a) Design a data pipeline to cleanse the raw data and produce a forecast for yearly\n",
    "carbon emissions from Baseline year to 2050. The forecast should follow a linear\n",
    "yearly decrease between baseline emissions and both interim target years, and\n",
    "ultimate net zero year. The forecast should be produced for each firm for each scope,\n",
    "as displayed in example output (A).\n",
    "\n",
    "b) Alongside this forecast data, produce ‘actual data’ to display the distance that each\n",
    "firm is from their current target.\n",
    "\n",
    "In our example output we have the following columns: \n",
    "\n",
    "-Lookup\n",
    "-verdantix_id\n",
    "-Actual/Projected\n",
    "-Year\n",
    "-Emmissions(tCO2e)\n",
    "\n",
    "Steps:\n",
    "1. Combine columns for each scope into one column\n",
    "2. Create a new dataframe to store the forecast emissions\n",
    "3. Create a new dataframe to store the actual emissions ( b )\n",
    "4. Merge the forecast and actual emissions dataframes\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task = merged_df.copy()\n",
    "df_task.drop(columns=['Scope 2 - Location Emissions', 'Scope 2 - market Emissions'], inplace=True)\n",
    "df_task.rename(columns={'Scope 2  Emissions': 'Scope 2 Emissions'}, inplace=True)\n",
    "df_task['Scope 3 Emissions Year'] = df_task['Scope 1 Emissions Year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 1. Combine columns for each scope into one column \"\"\"\n",
    "\n",
    "# Reduce the dataframe to combining Scope _1, Scope _2, and Scope _3 into a single Scope column\n",
    "reduced_df_1 = pd.melt(df_task, \n",
    "                       id_vars=[col for col in df_task.columns if col not in ['Scope _1', 'Scope _2', 'Scope _3']],\n",
    "                       value_vars=['Scope _1', 'Scope _2', 'Scope _3'],\n",
    "                       var_name='Scope', value_name='Scope Value'\n",
    "                       )\n",
    "\n",
    "# Replace the Scope column values to 1, 2, 3\n",
    "reduced_df_1['Scope'] = reduced_df_1['Scope'].map({'Scope _1': 1, 'Scope _2': 2, 'Scope _3': 3})\n",
    "\n",
    "# Drop the Scope Value column as it is not needed\n",
    "reduced_df_1.drop(columns=['Scope Value'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the 2022 emissions columns into a single column\n",
    "\n",
    "for year in range(2016, 2023):\n",
    "    reduced_df_1[f'{year} Emissions'] = reduced_df_1.apply(lambda row: row[f'{year}_{row[\"Scope\"]} Emissions'], axis=1)\n",
    "    reduced_df_1.drop(columns=[f'{year}_1 Emissions', f'{year}_2 Emissions', f'{year}_3 Emissions'], inplace=True)\n",
    "\n",
    "#reduced_df_1[reduced_df_1['verdantix_id'] == 'V000001']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_column_on_scope_number(df, new_col_name, old_col_names):\n",
    "\n",
    "    common_part = old_col_names[0].rsplit('_', 1)[0]\n",
    "\n",
    "    df[new_col_name] = df.apply(lambda row: row[f'{common_part}_{row[\"Scope\"]}'], axis=1)\n",
    "    df.drop(columns=old_col_names, inplace=True)\n",
    "    return df\n",
    "\n",
    "def combine_column_on_scope(df, new_col_name, old_col_names):\n",
    "\n",
    "    common_part = old_col_names[0].rsplit('-', 1)[0]\n",
    "\n",
    "    df[new_col_name] = df.apply(lambda row: row[f'{common_part} - Scope {row[\"Scope\"]}'], axis=1)\n",
    "    df.drop(columns=old_col_names, inplace=True)\n",
    "    return common_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'Scope 1 Emissions', 'Scope 2 Emissions', 'Scope 3 Emissions' into 'Emissions'\n",
    "reduced_df_1['Emissions'] = reduced_df_1.apply(lambda row: row[f'Scope {row[\"Scope\"]} Emissions'], axis=1)\n",
    "\n",
    "reduced_df_1.drop(columns=['Scope 1 Emissions', 'Scope 2 Emissions', 'Scope 3 Emissions'], inplace=True)\n",
    "\n",
    "\n",
    "# Combine 'Scope 1 Emissions Year', 'Scope 2 Emissions Year', 'Scope 3 Emissions Year' into 'Emission Year'\n",
    "reduced_df_1['Emission Year'] = reduced_df_1.apply(lambda row: row[f'Scope {row[\"Scope\"]} Emissions Year'], axis=1)\n",
    "\n",
    "reduced_df_1.drop(columns=['Scope 1 Emissions Year', 'Scope 2 Emissions Year', 'Scope 3 Emissions Year'], inplace=True)\n",
    "\n",
    "\n",
    "# Combine 'Baseline Year (Scope 1)', 'Baseline Year (Scope 2)', 'Baseline Year (Scope 3)' into 'Baseline Year'\n",
    "reduced_df_1['Baseline Year'] = reduced_df_1.apply(lambda row: row[f'Baseline Year (Scope {row[\"Scope\"]})'], axis=1)\n",
    "\n",
    "reduced_df_1.drop(columns=['Baseline Year (Scope 1)', 'Baseline Year (Scope 2)', 'Baseline Year (Scope 3)'], inplace=True)\n",
    "\n",
    "\n",
    "# Combine 'Net Zero Reduction Target Emissions - Scope 1', 'Net Zero Reduction Target Emissions - Scope 2', 'Net Zero Reduction Target Emissions - Scope 3' into 'Net Zero Reduction Target Emissions'\n",
    "reduced_df_1['Net Zero Reduction Target Emissions'] = reduced_df_1.apply(lambda row: row[f'Net Zero Reduction Target Emissions - Scope {row[\"Scope\"]}'], axis=1)\n",
    "\n",
    "reduced_df_1.drop(columns=[\n",
    "    'Net Zero Reduction Target Emissions - Scope 1',\n",
    "    'Net Zero Reduction Target Emissions - Scope 2',\n",
    "    'Net Zero Reduction Target Emissions - Scope 3'\n",
    "], inplace=True)\n",
    "\n",
    "\n",
    "# Combine 'Interim Target Emissions - Scope 1', 'Interim Target Emissions - Scope 2', 'Interim Target Emissions - Scope 3' into 'Interim Target Emissions'\n",
    "reduced_df_1['Interim Target Emissions'] = reduced_df_1.apply(lambda row: row[f'Interim Target Emissions - Scope {row[\"Scope\"]}'], axis=1)\n",
    "\n",
    "reduced_df_1.drop(columns=[\n",
    "    'Interim Target Emissions - Scope 1',\n",
    "    'Interim Target Emissions - Scope 2',\n",
    "    'Interim Target Emissions - Scope 3'\n",
    "], inplace=True)\n",
    "\n",
    "\n",
    "# Combine 'Interim Target Year 1: Scope 1', 'Interim Target Year 1: Scope 2', 'Interim Target Year 1: Scope 3' into 'Interim Target Year 1'\n",
    "reduced_df_1['Interim Target % 1'] = reduced_df_1.apply(lambda row: row[f'Interim Target % 1: Scope {row[\"Scope\"]}'], axis=1)\n",
    "\n",
    "reduced_df_1.drop(columns=[\n",
    "    'Interim Target % 1: Scope 1',\n",
    "    'Interim Target % 1: Scope 2',\n",
    "    'Interim Target % 1: Scope 3'\n",
    "], inplace=True)\n",
    "\n",
    "\n",
    "# Combine 'Base Year Emissions - Scope 1', 'Base Year Emissions - Scope 2', 'Base Year Emissions - Scope 3' into 'Base Year Emissions'\n",
    "reduced_df_1['Base Year Emissions'] = reduced_df_1.apply(lambda row: row[f'Base Year Emissions - Scope {row[\"Scope\"]}'], axis=1)\n",
    "\n",
    "reduced_df_1.drop(columns=[\n",
    "    'Base Year Emissions - Scope 1',\n",
    "    'Base Year Emissions - Scope 2',\n",
    "    'Base Year Emissions - Scope 3'\n",
    "], inplace=True)\n",
    "\n",
    "\n",
    "# Combine 'NZ Target Year: Scope 1', 'NZ Target Year: Scope 2', 'NZ Target Year: Scope 3' into 'NZ Target Year'\n",
    "reduced_df_1['NZ Target Year'] = reduced_df_1.apply(lambda row: row[f'NZ Target Year: Scope {row[\"Scope\"]}'], axis=1)\n",
    "\n",
    "reduced_df_1.drop(columns=[\n",
    "    'NZ Target Year: Scope 1',\n",
    "    'NZ Target Year: Scope 2',\n",
    "    'NZ Target Year: Scope 3'\n",
    "], inplace=True)\n",
    "\n",
    "\n",
    "# Combine 'Interim Target Year 1: Scope 1', 'Interim Target Year 1: Scope 2', 'Interim Target Year 1: Scope 3' into 'Interim Target Year 1'\n",
    "reduced_df_1['Interim Target Year 1'] = reduced_df_1.apply(lambda row: row[f'Interim Target Year 1: Scope {row[\"Scope\"]}'], axis=1)\n",
    "\n",
    "reduced_df_1.drop(columns=[\n",
    "    'Interim Target Year 1: Scope 1',\n",
    "    'Interim Target Year 1: Scope 2',\n",
    "    'Interim Target Year 1: Scope 3'\n",
    "], inplace=True)\n",
    "\n",
    "\n",
    "# Combine 'Actual/Projection_1', 'Actual/Projection_2', 'Actual/Projection_3' into 'Actual/Projection'\n",
    "reduced_df_1 = combine_column_on_scope_number(reduced_df_1, 'Actual/Projection', ['Actual/Projection_1','Actual/Projection_2','Actual/Projection_3'])\n",
    "\n",
    "\n",
    "# Combine 'lookup_1', 'lookup_2', 'lookup_3' into 'lookup'\n",
    "reduced_df_1 = combine_column_on_scope_number(reduced_df_1, 'lookup', ['lookup_1', 'lookup_2', 'lookup_3'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df_2 = reduced_df_1.copy()\n",
    "reduced_df_2.drop(columns=['2016 Emissions', '2017 Emissions', '2018 Emissions', '2019 Emissions', '2020 Emissions', '2021 Emissions', '2022 Emissions', 'Actual/Projection', 'Emissions', 'Emission Year'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 2. Create a new dataframe to store the forecast emissions \"\"\"\n",
    "\n",
    "# Create a new DataFrame to store the expanded rows\n",
    "expanded_rows = []\n",
    "\n",
    "# Loop through each row in the original DataFrame\n",
    "for _, row in reduced_df_2.iterrows():\n",
    "    base_year = row['Baseline Year']\n",
    "    net_zero_year = row['NZ Target Year']\n",
    "    for year in range(base_year, net_zero_year + 1):\n",
    "        new_row = row.copy()\n",
    "        new_row['Year'] = year\n",
    "        expanded_rows.append(new_row)\n",
    "\n",
    "# Create the expanded DataFrame\n",
    "expanded_df = pd.DataFrame(expanded_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new column 'Emissions (tCO2e)' \n",
    "expanded_df['Emissions (tCO2e)'] = expanded_df.apply(\n",
    "    lambda row: row['Base Year Emissions'] if row['Baseline Year'] == row['Year'] else\n",
    "                row['Interim Target Emissions'] if row['Interim Target Year 1'] == row['Year'] else\n",
    "                row['Base Year Emissions'] * 0.01 if row['NZ Target Year'] == row['Year'] else -1,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with the -1 values in the 'Emissions (tCO2e)' column\n",
    "expanded_df['Emissions (tCO2e)'] = expanded_df['Emissions (tCO2e)'].replace(-1.0, np.nan)\n",
    "\n",
    "# Interpolate the missing values in the 'Emissions (tCO2e)' column\n",
    "expanded_df['Emissions (tCO2e)'] = expanded_df['Emissions (tCO2e)'].interpolate(method='linear')\n",
    "\n",
    "# Create the new column 'Actual/Projection'\n",
    "expanded_df['Actual/Projection'] = 'Projection'\n",
    "\n",
    "# Drop columns that are no longer needed\n",
    "expanded_df.drop(columns=['Net Zero Reduction Target Emissions','Interim Target Emissions',\t'Interim Target % 1', 'Base Year Emissions', 'NZ Target Year', 'Interim Target Year 1', 'Baseline Year'], inplace=True)\n",
    "\n",
    "# Rearrange the columns\n",
    "expanded_df_projected = expanded_df[['lookup', 'verdantix_id', 'Scope', 'Actual/Projection', 'Year', 'Emissions (tCO2e)']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 3. Create a new dataframe to store the actual emissions \"\"\"\n",
    "# Reformat the actual emissions\n",
    "\n",
    "expanded_df_actual = reduced_df_1.drop(columns=['Net Zero Reduction Target Emissions','Interim Target Emissions', 'Interim Target % 1', 'Base Year Emissions','NZ Target Year', 'Interim Target Year 1','Emissions', 'Emission Year', 'Baseline Year'])\n",
    "expanded_df_actual = pd.melt(expanded_df_actual, id_vars=['verdantix_id', 'Scope', 'Actual/Projection', 'lookup'], var_name='Year', value_name='Emissions (tCO2e)')\n",
    "\n",
    "expanded_df_actual['Year'] = expanded_df_actual['Year'].str.extract(r'(\\d{4})')\n",
    "\n",
    "# Remove rows where emissions are -1\n",
    "expanded_df_actual = expanded_df_actual[expanded_df_actual['Emissions (tCO2e)'] != -1]\n",
    "\n",
    "\n",
    "# Rearrange the columns\n",
    "expanded_df_actual = expanded_df_actual[['lookup', 'verdantix_id', 'Scope', 'Actual/Projection', 'Year', 'Emissions (tCO2e)']]\n",
    "\n",
    "expanded_df_actual['Emissions (tCO2e)'] = expanded_df_actual['Emissions (tCO2e)'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 4. Merge the forecast and actual emissions dataframes \"\"\"\n",
    "# Combine the projected and actual emissions df\n",
    "\n",
    "final_df = pd.concat([expanded_df_projected, expanded_df_actual],axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('final_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\" \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mTask \u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mfinal_df\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'final_df' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Task \n",
    "\n",
    "c) Produce an ‘implied overshoot’ forecast which plots the projected net zero year for\n",
    "corporates whose actual emissions are higher than forecast emissions for the most\n",
    "recent year.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
